{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Selene\n",
    "\n",
    "This tutorial explores the core components of Selene, and should teach you everything you need to know to train a simple model on biological sequence data. \n",
    "Before starting this tutorial, you need to install Selene.\n",
    "Instructions for installation are available [here](https://selene.flatironinstitute.org/installation.html).\n",
    "Lastly, if you are not familiar with neural networks, we recommend reading through this [introductory PyTorch tutorial on neural networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n",
    "In the simplest case, we train a neural network as follows:\n",
    "\n",
    "1. Construct our neural network, which should be a [`torch.nn.Module`](https://pytorch.org/docs/stable/nn .html#torch.nn.Module) object\n",
    "2. Load the training data and divide it into training and validation sets\n",
    "3. Iterate over the training set\n",
    "4. Compute and backpropagate the training loss after each iteration\n",
    "5. Save the model weights at specified intervals\n",
    "6. Compute and report the loss on the validation set at specified intervals\n",
    "7. Compute and report the loss on the validation set after training is complete\n",
    "\n",
    "In this case, much of our work is already done for us.\n",
    "In fact, we do not actually need to write any code besides our model and a configuration file.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "In addition to the `selene-sdk` library, we also use [`htslib`](http://www.htslib.org) for data processing. Specifically, we use the tools [**`tabix`**](http://www.htslib.org/doc/tabix.html) and [**`bgzip`**](http://www.htslib.org/doc/bgzip.html) from `htslib`. \n",
    "\n",
    "If you eventually want to use Selene on new data, please download `htslib`. We recommend using `selene-sdk` in a [conda environment](https://conda.io/docs/user-guide/tasks/manage-environments.html) and installing `htslib` (`conda install htslib -c bioconda`) to that same environment. \n",
    "\n",
    "However, if you take this approach, you can only use `tabix` and `bgzip` on the command-line when the conda environment is activated. You can instead download and build the package on your machine by following the instructions on the [`htslib` website](http://www.htslib.org/download/). \n",
    "\n",
    "## Download and format the data\n",
    "\n",
    "First, we need to download the data. (We provide a \"Shortcut\" below that contains all the formatted data in a `.tar.gz`.)\n",
    "\n",
    "In this tutorial, we can go through a single-feature example with [data](http://hgdownload.cse.ucsc.edu/goldenpath/hg19/encodeDCC/wgEncodeAwgTfbsUniform/) from the ENCODE Uniform TFBS composite track. This is the transcription factor dataset that was used to train [Zhou and Troyanskaya's (2015)](https://doi.org/10.1038/nmeth.3547) DeepSEA model.\n",
    "\n",
    "We can download the measurements for transcription factor CTCF in cell type GM12878 by running\n",
    "\n",
    "```sh\n",
    "wget http://hgdownload.cse.ucsc.edu/goldenpath/hg19/encodeDCC/wgEncodeAwgTfbsUniform/wgEncodeAwgTfbsUtaGm12878CtcfUniPk.narrowPeak.gz\n",
    "```\n",
    "\n",
    "and format the data with\n",
    "\n",
    "```sh\n",
    "bgzip -d wgEncodeAwgTfbsUtaGm12878CtcfUniPk.narrowPeak.gz\n",
    "\n",
    "cut -f 1-3 wgEncodeAwgTfbsUtaGm12878CtcfUniPk.narrowPeak > GM12878_CTCF.bed\n",
    "\n",
    "sed -i \"s/$/\\tGM12878|CTCF|None/\" GM12878_CTCF.bed\n",
    "\n",
    "sort -k1V -k2n -k3n GM12878_CTCF.bed > sorted_GM12878_CTCF.bed\n",
    "```\n",
    "\n",
    "The formatted BED file should contain 4 columns, in order: chromosome, start, end, feature. We do not support strand-specific data at this time (it will be added in the next version of Selene).\n",
    "\n",
    "In this example, we will use the `IntervalsSampler` class for partitioning and sampling the data. The intervals sampler requires that we pass in an intervals file with 3 columns: chrom, start, end. This intervals file determines where in the genome we sample our data. We have provided an intervals file for you with the regions in the original DeepSEA dataset that contained at least 1 transcription factor (TF). We will refer to this file as \"DeepSEA TF regions\" from now on.\n",
    "\n",
    "It also requires that we tabix-index the dataset BED file for fast querying of targets in genomic regions.\n",
    "\n",
    "```sh\n",
    "bgzip -c sorted_GM12878_CTCF.bed > sorted_GM12878_CTCF.bed.gz\n",
    "\n",
    "tabix -p bed sorted_GM12878_CTCF.bed.gz\n",
    "```\n",
    "\n",
    "Selene provides sampler classes to partition your dataset into training/testing/validation sets and will draw examples from the appropriate partitions during the training/evaluation process.\n",
    "\n",
    "These sampler classes require that you have a file containing the distinct genomic features that the model predicts. Note that when we refer to a model's \"features\", we are referring to the genomic features that it predicts (i.e. they are the same as classes, labels, or targets that a deep learning model predicts). \n",
    "\n",
    "```sh\n",
    "cut -f 4 sorted_GM12878_CTCF.bed | sort -u > distinct_features.txt\n",
    "```\n",
    "\n",
    "Finally, we must download the hg19 FASTA file:\n",
    "\n",
    "```sh\n",
    "wget https://www.encodeproject.org/files/male.hg19/@@download/male.hg19.fasta.gz\n",
    "\n",
    "bgzip -d male.hg19.fasta.gz\n",
    "```\n",
    "\n",
    "### SHORTCUT: download all formatted data from Zenodo record\n",
    "\n",
    "Download the compressed data from here:\n",
    "\n",
    "```sh\n",
    "wget https://zenodo.org/record/1443558/files/selene_quickstart.tar.gz\n",
    "```\n",
    "\n",
    "Extract it and `mv` all files from the extracted directory `selene_quickstart_tutorial` to the current directory.\n",
    "\n",
    "## Command line arguments\n",
    "\n",
    "At the end of this tutorial, we run Selene using library functions. These are the same functions used in Selene's [command-line interface (CLI)](https://github.com/FunctionLab/selene/blob/master/selene_cli.py), a file that users can copy and use after installing Selene or cloning and building the local version. \n",
    "\n",
    "If you use the CLI script, please install `docopt` (`conda install docopt`), which the CLI relies on to parse input arguments.\n",
    "\n",
    "Selene uses a limited number (two to be precise) of command line arguments.\n",
    "The first of these is the positional parameter for the configuration file, which we will discuss in more detail in the following section.\n",
    "The second argument is the optional named argument for the learning rate, specified with `--lr`.\n",
    "The learning rate only needs to be specified when Selene is training a model, and is ignored in all other circumstances.\n",
    "\n",
    "If you install Selene via conda or pip (we recommend conda), only download and use the CLI script--there is no need to clone the entire repository. You should only do so if you plan on developing or modifying Selene yourself.\n",
    "\n",
    "## Configuration file syntax\n",
    "\n",
    "The configuration file is a [YAML file](https://en.wikipedia.org/wiki/YAML) that specifies the majority of the runtime parameters for Selene.\n",
    "In general, a YAML file with keys `key1` and `key2` taking values `val1` and `val2` would look like such:\n",
    "\n",
    "```YAML\n",
    "---\n",
    "key1: val1\n",
    "key2: val2\n",
    "...\n",
    "```\n",
    "\n",
    "For training a new network, there are a few keys that we must include in this YAML file, which we will discuss later.\n",
    "\n",
    "The following sections explain each of these parameters in some detail.\n",
    "However, we first need to discuss the syntax for our configuration file.\n",
    "We discuss each of the argument types for configuration files below.\n",
    "\n",
    "### Literal arguments\n",
    "\n",
    "The simplest configuration arguments are literals.\n",
    "To specify a learning rate of `0.01`, and that we would like to evaluate test metric performance and not save the datasets, we would include the following lines in our configuration file:\n",
    "```YAML\n",
    "lr: 0.01\n",
    "random_seed: 123\n",
    "output_dir: path/to/output/dir\n",
    "```\n",
    "\n",
    "Note that at the top-level, we do not separate these arguments with commas. \n",
    "\n",
    "### List arguments\n",
    "\n",
    "After literals, lists arguments like `ops` are the next simplest type of configuration parameter.\n",
    "Syntactically, list arguments are very similar to the python lists that they represent.\n",
    "For instance, to specify `ops` as the Python list below:\n",
    "```python\n",
    "ops = [\"train\"]\n",
    "```\n",
    "we would write the following line in our configuration file:\n",
    "```YAML\n",
    "ops: [train]\n",
    "```\n",
    "\n",
    "### Dictionary arguments\n",
    "\n",
    "The next type of argument we need is a dictionary.\n",
    "Like lists, dictionaries in the configuration file are very similar to their Python equivalents.\n",
    "For instance, if the `model` configuration were written as a dictionary in Python, it might look something like the following:\n",
    "```python\n",
    "model = {\"file\": \"/absolute/path/to/deeperdeepsea.py\",\n",
    "         \"class\": \"DeeperDeepSEA\",\n",
    "         \"sequence_length\": 1000\n",
    "         \"n_classes_to_predict\": 1,\n",
    "         \"non_strand_specific\": {\n",
    "             \"use_module\": True,\n",
    "             \"mode\": \"mean\"\n",
    "          }\n",
    "        }\n",
    "```\n",
    "Now, to write this in the configuration file, we simply include the following lines:\n",
    "```YAML\n",
    "model: {\n",
    "    file: /absolute/path/to/deepsea.py,\n",
    "    class: DeeperDeepSEA,\n",
    "    sequence_length: 1000,\n",
    "    n_classes_to_predict: 1,\n",
    "    non_strand_specific: {\n",
    "        use_module: True,\n",
    "        mode: mean\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Function arguments \n",
    "\n",
    "In addition to the types we've just discussed, Selene's configuration accept python function calls.\n",
    "For instance, let's say we want to specify the value of the `features` argument for `train_model`, which takes a list of strings and specifies the names of the values we are predicting with our model.\n",
    "One option would be to write the list of strings into the configuration file, but this might take a long time if this list is very long.\n",
    "If we were using Python, we would just read the list of feature names the following:\n",
    "```python\n",
    "import selene_sdk\n",
    "features = selene_sdk.utils.load_features_list(input_path=\"distinct_features.txt\")\n",
    "```\n",
    "Fortunately, we can use function the function call arguments to include this in our configuration file.\n",
    "Specifically, we would write the following in our configuration file:\n",
    "```YAML\n",
    "features: !obj.selene_sdk.utils.load_features_list {\n",
    "    input_path: <path>/distinct_features.txt\n",
    "}\n",
    "```\n",
    "\n",
    "## Training a model and analyzing sequences with it\n",
    "\n",
    "To train or analyze sequences with a model, we first specify the configuration file and then we execute `selene` from the command line.\n",
    "The first section provides an overview of all the requirements for training a model with Selene.\n",
    "The second section covers the arguments used to evaluate sequences with a trained model.\n",
    "We recommend opening the included `simple_train.yml` configuration file and following along in them while reading through these sections.\n",
    "\n",
    "### Configuration file arguments for training\n",
    "Before running Selene from the command line, we need to specify its runtime parameters in a configuration file.\n",
    "Specifically, we need to include the following:\n",
    "\n",
    "| key              | definition |\n",
    "|------------------|-----------------------------------------------------------------------------------------------------|\n",
    "| ops              | list of operations to execute with Selene |\n",
    "| model            | dict containing the configuration parameters for the model we intend to train.\n",
    "| sampler          | a subclass of selene_sdk.samplers.Sampler |\n",
    "| train_model      | a subclass of selene_sdk.TrainModel |\n",
    "| lr               | a floating point value for the learning rate, if we do not want to specify it in the command line arguments |\n",
    "| evaluate_on_test | a boolean specifying whether we should calculate performance metrics on held out test data |\n",
    "| save_datasets    | a boolean specifying if we would like to write the training/validation/test data to file                   |\n",
    "\n",
    "#### ops\n",
    "\n",
    "Selene currently supports the `train` and `analyze` operations, and allows chaining of operations by simply adding them to the `ops` list in the configuration file.\n",
    "For instance, to train a model and then use it to analyze some data, you would include the following line in the configuration file:\n",
    "```YAML\n",
    "ops: [train, analyze]\n",
    "```\n",
    "To only train a model, we would just write the following:\n",
    "```YAML\n",
    "ops: [train]\n",
    "```\n",
    "\n",
    "Each individual operation comes with its own requirements about what keys are in the configuration file. These keys contain the class configurations required to run a given operation. We have examples of configuration files [here](https://github.com/FunctionLab/selene/tree/master/config_examples) for the main operations that you can run with Selene.\n",
    "\n",
    "#### model\n",
    "\n",
    "In this tutorial, we will use an example neural network from [DeepSEA](http://deepsea.princeton.edu), which models chromatin properties of sequences in the non-coding genome.\n",
    "The class for this model, `DeepSEA`, is specified in the `deepsea.py` file from earlier.\n",
    "The model should follow all of the [normal rules for specifying a `torch.nn.Module`](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html), with two exceptions.\n",
    "First, the file with the model class should include a method called `criterion` that returns the object to use for the [PyTorch loss function](https://pytorch.org/docs/master/nn.html#loss-functions).\n",
    "In `deepsea.py`, this is defined as follows:\n",
    "```python\n",
    "def criterion():\n",
    "    return torch.nn.BCELoss()\n",
    "```\n",
    "Second, we must define a method called `get_optimizer` that takes a learning rate, and returns the [optimization function](https://pytorch.org/docs/master/optim.html) and its parameters.\n",
    "The return value should be a 2-tuple, where the first element is the optimizer class, and the second element is a `dict` containing the keyword arguments to use when constructing the optimizer.\n",
    "In `deepsea.py`, this is specified as follows:\n",
    "```python\n",
    "def get_optimizer(lr):\n",
    "    return (torch.optim.SGD, {\"lr\": lr, \"weight_decay\": 1e-6, \"momentum\": 0.9})\n",
    "```\n",
    "Note that, to allow specifying the learning rate at the command line, you should include the passed `lr` argument in the `dict` of keyword arguments.\n",
    "\n",
    "#### sampler\n",
    "\n",
    "The `sampler` argument specifies how Selene will sample its training data.\n",
    "The value for `sampler` should be a function-type argument, and the function needed to construct an object that is a subclass of `selene_sdk.samplers.Sampler`. \n",
    "The specific arguments for the sampler's construction will vary by class, so it is important to check the class definitions and documentation when specifying them.\n",
    "For the example, we will use the following configuration for the `sampler`:\n",
    "```YAML\n",
    "sampler: !obj:selene_sdk.samplers.IntervalsSampler {\n",
    "    reference_sequence: !obj:selene_sdk.sequences.Genome {\n",
    "        input_path: male.hg19.fasta\n",
    "    },\n",
    "    features: !obj:selene_sdk.utils.load_features_list {\n",
    "        input_path: distinct_features.txt\n",
    "    },\n",
    "    target_path: sorted_GM12878_CTCF.bed.gz,\n",
    "    intervals_path: deepsea_TF_intervals.txt,\n",
    "    seed: 127,\n",
    "    sample_negative: True,\n",
    "    sequence_length: 1000,\n",
    "    center_bin_to_predict: 200,\n",
    "    test_holdout: [chr8, chr9],\n",
    "    validation_holdout: [chr6, chr7],\n",
    "    feature_thresholds: 0.5,\n",
    "    mode: train,\n",
    "    save_datasets: [test]\n",
    "}\n",
    "```\n",
    "\n",
    "The intervals sampler samples from regions specified in `intervals_path`. In this case, we provide you with the regions in the DeepSEA dataset that contained at least 1 transcription factor. \n",
    "\n",
    "For your own dataset, you might do something similar. Alternatively, you could sample uniformly from all regions in the genome. If you want to same uniformly across the genome, please look at the documentation for [`selene_sdk.samplers.RandomPositionsSampler`](http://selene.flatironinstitute.org/samplers.html).\n",
    "\n",
    "#### train_model\n",
    "The `train_model` argument is responsible to specifying many of the parameters for `selene_sdk.TrainModel`.\n",
    "The following parameters for `train_model` are automatically generated, and should not be specified in the configuration file:\n",
    "\n",
    "|                |\n",
    "|----------------|\n",
    "| model          |\n",
    "|data_sampler    |\n",
    "|loss_criterion  |\n",
    "|optimizer_class |\n",
    "|optimizer_kwargs|\n",
    "\n",
    "With this in mind, we write the following in our configuration file:\n",
    "```YAML\n",
    "train_model: !obj:selene_sdk.TrainModel {\n",
    "    batch_size: 64,\n",
    "    # typically the number of steps is much higher\n",
    "    max_steps: 16000,  \n",
    "    # the number of mini-batches the model should sample before reporting performance\n",
    "    report_stats_every_n_steps: 2000,\n",
    "    n_validation_samples: 32000,\n",
    "    n_test_samples: 120000,\n",
    "    cpu_n_threads: 32,\n",
    "    use_cuda: False,\n",
    "    data_parallel: False\n",
    "}\n",
    "```\n",
    "\n",
    "#### other arguments\n",
    "\n",
    "There are three additional optional arguments we need when training models: `lr`, `output_dir`, and `create_subdirectory`.\n",
    "If you do not want to specify the learning rate in the command line arguments, you can specify it in the configuration file.\n",
    "However, note that Selene will throw an exception and crash if training is specified in the operations `op` and `lr` is not included in the configuration file or specified in the command line arguments.\n",
    "If we want to specify it in the configuration file, we can include the following lines:\n",
    "```YAML\n",
    "lr: 0.01\n",
    "```\n",
    "\n",
    "It is recommended that you specify a random seed for `torch`. This is helpful for reproducibility. \n",
    "```YAML\n",
    "random_seed: 1447\n",
    "```\n",
    "Lastly, we would include the path to an output directory in the configuration file. If the directory does not exist, it is automatically created for you. \n",
    "```YAML\n",
    "output_dir: ./training_outputs\n",
    "```\n",
    "\n",
    "Finally, if `create_subdirectory` is True, Selene will create a subdirectory with the current timestamp as the name within `output_directory`. This is so that you can save multiple runs in a single directory:\n",
    "```YAML\n",
    "create_subdirectory: True\n",
    "```\n",
    "\n",
    "### Running it\n",
    "\n",
    "Now, it only takes 2 methods to run training in Selene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from selene_sdk.utils import load_path\n",
    "from selene_sdk.utils import parse_configs_and_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the configuration file as a dictionary. We provide `./simple_train.yml` for you, but you must review this file and the spots labeled `# TODO` before running the subsequent cells. For example, you need to replace the path in `model[\"file\"]` with the absolute path to the model architecture file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = load_path(\"./simple_train.yml\", instantiate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, execute the operations that were specified in `ops`. This configuration parsing function will look for the classes/parameters corresponding to the operations \"train\" and \"evaluate\", instantiate those objects, and run the necessary methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs and logs saved to ./training_outputs\n"
     ]
    }
   ],
   "source": [
    "parse_configs_and_run(configs, lr=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help section (FAQs)\n",
    "\n",
    "#### How to select `n_steps` (how online sampling works)\n",
    "\n",
    "If you are using the `IntervalsSampler` (like we do in this example), you have passed in a list of regions from which you'd like to sample. If you are using `RandomPositionsSampler`, you are using the entire genome (minus any blacklist regions, see documentation [here](http://selene.flatironinstitute.org/sequences.html#genome)) for sampling. \n",
    "\n",
    "An `OnlineSampler` will randomly select a position from the regions you'd like to sample and query the reference sequence for a sequence of `sequence_length` bp centered at that position. \n",
    "\n",
    "When selecting `n_steps`, you should consider the number of possible positions from which the online sampler can sample, and how many times you might want to re-sample from that set. \n",
    "\n",
    "#### How to select: `report_stats_every_n_steps`, `n_validation_samples`, and `n_test_samples`\n",
    "\n",
    "We currently report ROC AUC and average precision averaged across all classes (e.g. genomic features) predicted by the model for both the validation and the test sets.\n",
    "\n",
    "`report_stats_every_n_steps` may be renamed in the future--when working with an online sampler, it can be used to report the validation ROC AUC and average precision after every estimated \"epoch\". You can set it to report more frequently than this if you'd like. \n",
    "\n",
    "Both `n_validation_samples` and `n_test_samples` should be larger enough such that we expect to sample positive examples for each feature. In the tutorial example here, we have set `sample_negative` to `True` because our model is only classifying whether a sequence contains 1 genomic feature or not. In a multi-class example, we may not need to do this because a positive example for 1 class might be a negative example for all other classes. In any case, you should estimate the number of samples you might randomly sample from regions in the genome to get to at least 1 positive example. **Otherwise, `roc_auc` and `average_precision` will always be reported as `None`** because all your samples will be labeled with 0 classes/genomic features.\n",
    "\n",
    "Selene's `TrainModel` class by default sets `report_gt_feature_n_positives` to 10. This means you must have at least 10 positive examples for a genomic feature for the ROC AUC and average precision to be computed. \n",
    "\n",
    "#### Troubleshooting: no log statements being printed to the log file\n",
    "\n",
    "No log statements are printed while the sampler partitions the data into training, validation, and testing. If `n_validation_samples` and/or `n_test_samples` are very large, training may not start for quite a while. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
